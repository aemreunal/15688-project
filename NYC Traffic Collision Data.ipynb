{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Analyzing NYC Traffic Collision Data</center>\n",
    "## <center>A 15-688 Project by:</center><center><br/> Ahmet Emre Unal (ahmetemu)<br/><br/>Marco Peyrot (mpeyrotc)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New York City is a wonderful city with terrible traffic. The drivers are impatient and aggressive. This results in many traffic collisions every day, some of which, quite unfortunately, lead to injuries and death. \n",
    "\n",
    "For our project, we wanted to understand NYC's most collision-prone areas and try to predict collisions based on many factors, such as location, vehicle type, whether it's a weekday, etc.\n",
    "\n",
    "The [NYPD Motor Vehicle Collisions](https://data.cityofnewyork.us/Public-Safety/NYPD-Motor-Vehicle-Collisions/h9gi-nx95/data) dataset we found was surprisingly feature-rich and populous. It also meant that there were lots of collision instances with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests \n",
    "import scipy\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "matplotlib.use('svg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "raw_data_file_name = 'NYPD_Motor_Vehicle_Collisions.csv'\n",
    "bicycle_lanes_page_url = 'http://www.nyc.gov/html/dot/html/bicyclists/lane-list.shtml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started by cleaning up rows with empty and '`UNKNOWN`' values. This threw away about a third of the original dataset. We then dropped columns that either were unnecessary (like the reason for the crash, since this sort of information is not possible to infer prior to the crash, like the driver being distracted) or were too detailed (like the vehicle subtypes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    collision = pd.read_csv(file_name, \n",
    "                            na_filter=False, \n",
    "                            parse_dates={'DATE_COMBINED' : ['DATE', 'TIME']}, \n",
    "                            infer_datetime_format=True)\n",
    "    \n",
    "    # Remove rows that don't have the necessary data\n",
    "    columns_to_check_for_empty = ['LOCATION', 'LATITUDE', 'LONGITUDE', 'BOROUGH',\n",
    "                                  'ZIP CODE', 'ON STREET NAME', 'CROSS STREET NAME', \n",
    "                                  'VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 1']\n",
    "    for column in columns_to_check_for_empty:\n",
    "        collision = collision[collision[column] != '']\n",
    "        collision = collision[collision[column] != 'UNKNOWN']\n",
    "\n",
    "    # Drop unneeded columns\n",
    "    columns_to_drop = ['CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2', \n",
    "                       'CONTRIBUTING FACTOR VEHICLE 3', 'CONTRIBUTING FACTOR VEHICLE 4', \n",
    "                       'CONTRIBUTING FACTOR VEHICLE 5', 'LOCATION', 'OFF STREET NAME', \n",
    "                       'VEHICLE TYPE CODE 2', 'VEHICLE TYPE CODE 3', 'VEHICLE TYPE CODE 4', \n",
    "                       'VEHICLE TYPE CODE 5']\n",
    "    for column in columns_to_drop:\n",
    "        collision = collision.drop(column, axis=1)\n",
    "        \n",
    "    # Set column types\n",
    "    collision['ZIP CODE'] = collision['ZIP CODE'].astype(int)\n",
    "    collision['LATITUDE'] = collision['LATITUDE'].astype(float)\n",
    "    collision['LONGITUDE'] = collision['LONGITUDE'].astype(float)\n",
    "    \n",
    "    # Rename date column to just 'DATE'\n",
    "    collision = collision.rename(columns={'DATE_COMBINED':'DATE'})\n",
    "    \n",
    "    # Eliminate duplicates\n",
    "    collision = collision.drop_duplicates()\n",
    "    \n",
    "    # Reset index\n",
    "    collision = collision.reset_index(drop=True)\n",
    "    \n",
    "    return collision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceeded to create two temporal features:\n",
    "\n",
    " 1. Whether it occurred on a weekday or weekend\n",
    " 2. What 'time period' in the day it occurred\n",
    "\n",
    "The 'time period' requires some explanation: We all know that morning and evening rush hours can be especially more collision-prone than day time (around noon) and night time (after the evening rush and before the morning rush). We decided to create these four bins with the following time ranges:\n",
    "\n",
    " 1. Night Time: 00:00-06:59 & 20:00-00:00\n",
    " 2. Morning Rush: 07:00-10:59\n",
    " 3. Night Time: 00:00-06:59 & 20:00-00:00\n",
    " 4. Evening Rush: 16:00-19:59\n",
    "\n",
    "Together with the weekday/weekend feature, this allowed us to represent the date in a way that is meaningful to a machine learning algorithm. We also assumed that, as long as it's a weekday, the exact day (whether it's a Monday or a Thursday) is not significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_temporal_features(collision):\n",
    "    collision = _create_time_features(collision)\n",
    "    collision = _create_date_features(collision)\n",
    "    \n",
    "    # We're done with date, we can drop it\n",
    "    collision = collision.drop('DATE', axis=1)\n",
    "    \n",
    "    return collision\n",
    "    \n",
    "\n",
    "def _create_time_features(collision):\n",
    "    # Create one-hot time of day representation\n",
    "    ## Date part is unimportant\n",
    "    morning_rush_begin = pd.datetime(2000, 01, 01, 07, 00, 00).time()\n",
    "    morning_rush_end = pd.datetime(2000, 01, 01, 11, 00, 00).time()\n",
    "    evening_rush_begin = pd.datetime(2000, 01, 01, 16, 00, 00).time()\n",
    "    evening_rush_end = pd.datetime(2000, 01, 01, 20, 00, 00).time()\n",
    "    collision_time = collision['DATE'].dt.time\n",
    "    \n",
    "    ## Night Time 00:00-06:59 & 20:00-00:00\n",
    "    night_time = (collision_time >= evening_rush_end) | (collision_time < morning_rush_begin)\n",
    "    night_time_onehot = pd.get_dummies(night_time).loc[:, True].astype(int)\n",
    "    collision = collision.assign(NIGHT_TIME = night_time_onehot.values)\n",
    "    \n",
    "    ## Morning Rush 07:00-10:59\n",
    "    morning_rush = (collision_time >= morning_rush_begin) & (collision_time < morning_rush_end)\n",
    "    morning_rush_onehot = pd.get_dummies(morning_rush).loc[:, True].astype(int)\n",
    "    collision = collision.assign(MORNING_RUSH = morning_rush_onehot.values)\n",
    "    \n",
    "    ## Night time 00:00-06:59 & 20:00-00:00\n",
    "    day_time = (collision_time >= morning_rush_end) & (collision_time < evening_rush_begin)\n",
    "    day_time_onehot = pd.get_dummies(day_time).loc[:, True].astype(int)\n",
    "    collision = collision.assign(DAY_TIME = day_time_onehot.values)\n",
    "    \n",
    "    ## Evening Rush 16:00-19:59\n",
    "    evening_rush = (collision_time >= evening_rush_begin) & (collision_time < evening_rush_end)\n",
    "    evening_rush_onehot = pd.get_dummies(evening_rush).loc[:, True].astype(int)\n",
    "    collision = collision.assign(EVENING_RUSH = evening_rush_onehot.values)\n",
    "    \n",
    "    return collision\n",
    "\n",
    "def _create_date_features(collision):\n",
    "    # Create one-hot weekday/weekend representation\n",
    "    collision_day = collision['DATE'].dt.dayofweek\n",
    "    \n",
    "    ## Weekday 0, 1, 2, 3, 4\n",
    "    ## Weekend 5, 6\n",
    "    is_weekday = (collision_day <= 4)\n",
    "    is_weekday_onehot = pd.get_dummies(is_weekday).astype(int)\n",
    "    \n",
    "    ## Weekday\n",
    "    weekday_onehot = is_weekday_onehot.loc[:, True]\n",
    "    collision = collision.assign(WEEKDAY = weekday_onehot.values)\n",
    "\n",
    "    ## Weekend\n",
    "    weekend_onehot = is_weekday_onehot.loc[:, False]\n",
    "    collision = collision.assign(WEEKEND = weekend_onehot.values)\n",
    "    \n",
    "    return collision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further proceeded to create a one-hot encoding of the vehicle types that were available in the dataset. NYPD has put almost half of the vehicles in a general group called '`PASSENGER VEHICLE`', which seems to represent the common sedan type of vehicle. Other types, like SUVs, motorcycles, small and large commercial vehicles, have their own types. \n",
    "\n",
    "We removed some collision types (like the collisions in which the vehicle was an '`AMBULANCE`') with very few collisions. This further cut about 5% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vehicle_features(collision):\n",
    "    # Create one-hot vehicle type representation\n",
    "    vehicle_types_onehot = pd.get_dummies(collision.loc[:, 'VEHICLE TYPE CODE 1']).astype(int)\n",
    "    \n",
    "    # Merge Motorcycle & Scooter columns\n",
    "    motorcycle = vehicle_types_onehot.loc[:, 'MOTORCYCLE'] + vehicle_types_onehot.loc[:, 'SCOOTER']\n",
    "    vehicle_types_onehot = vehicle_types_onehot.drop('MOTORCYCLE', axis=1)\n",
    "    vehicle_types_onehot = vehicle_types_onehot.drop('SCOOTER', axis=1)\n",
    "    vehicle_types_onehot = vehicle_types_onehot.assign(MOTORCYCLE = motorcycle.values)\n",
    "    \n",
    "    # Concatanate one-hot with collisions\n",
    "    collision = pd.concat([collision, vehicle_types_onehot], axis=1)\n",
    "    \n",
    "    # Drop unneeded collisions\n",
    "    vehicles_to_drop = ['OTHER', 'AMBULANCE', 'PEDICAB', 'FIRE TRUCK', 'LIVERY VEHICLE']\n",
    "    collisions_to_drop = vehicle_types_onehot.loc[:, vehicles_to_drop[0]]\n",
    "    for i in xrange(1, len(vehicles_to_drop)):  # Start from 1 since we already have OTHER\n",
    "        collisions_to_drop += vehicle_types_onehot.loc[:, vehicles_to_drop[i]]\n",
    "    collisions_to_keep = (collisions_to_drop == 0)\n",
    "    collision = collision[collisions_to_keep]\n",
    "    collision = collision.reset_index(drop=True)  # Reset index due to dropped rows\n",
    "    \n",
    "    # Drop unneeded vehicle columns\n",
    "    for column in vehicles_to_drop:\n",
    "        collision = collision.drop(column, axis=1)\n",
    "    \n",
    "    # Drop vehicle type column\n",
    "    collision = collision.drop('VEHICLE TYPE CODE 1', axis=1)\n",
    "        \n",
    "    return collision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the collision data\n",
    "collision = load_data(raw_data_file_name)\n",
    "collision = create_temporal_features(collision)\n",
    "collision = create_vehicle_features(collision)\n",
    "\n",
    "print collision.head()\n",
    "print collision.dtypes\n",
    "print len(collision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed with parsing NYC's [bike lanes list web page](http://www.nyc.gov/html/dot/html/bicyclists/lane-list.shtml) to add another feature to our collision data: whether the street the accident occurred in had a bicycle lane or not. We do that by manually parsing the bicycle lanes page and matching them to the street names of collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_bicycle_lanes_from_page(page_html):\n",
    "    \"\"\"\n",
    "    Parse the table contained in the bicycle lanes webpage.\n",
    "\n",
    "    Args:\n",
    "        page_html (string): String of HTML corresponding to the data source webpage.\n",
    "\n",
    "    Returns:\n",
    "        a dictionary that contains mappings from a category to the list containing the data.\n",
    "        These categories are: street, begins, ends, and borough.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    bicycle_lanes_list = {\n",
    "        'street': [],\n",
    "        'begins': [],\n",
    "        'ends': [],\n",
    "        'borough': []\n",
    "    }\n",
    "\n",
    "    table = soup.findChildren('tbody')[0]\n",
    "    rows = table.findChildren(['tr'])\n",
    "    \n",
    "    for row in rows:\n",
    "        cells = row.findChildren('td')\n",
    "        content = cells[0].text\n",
    "        m = re.search(r'^([a-zA-Z\\s0-9\\-\\.,\\(\\)]+) (from)*(between)* '\n",
    "                      r'([a-zA-Z\\s0-9\\-\\.,\\(\\)]+) (to)*(and)* '\n",
    "                      r'([a-zA-Z\\s0-9\\-\\.,\\(\\)]+)$', content)\n",
    "        \n",
    "        # Content that does not follow this syntax is discarded because\n",
    "        # it refers to landscapes or parks.\n",
    "        if m is not None:\n",
    "            bicycle_lanes_list['street'].append(m.group(1).upper())\n",
    "            bicycle_lanes_list['begins'].append(m.group(4).upper())\n",
    "            bicycle_lanes_list['ends'].append(m.group(7).upper())\n",
    "            bicycle_lanes_list['borough'].append(cells[2].text.upper())\n",
    "\n",
    "    return bicycle_lanes_list\n",
    "    \n",
    "def extract_bicycle_lanes(url):\n",
    "    \"\"\"\n",
    "    Retrieve all of the bicycle lane information for the city of New York.\n",
    "\n",
    "    Parameters:\n",
    "        url (string): page URL corresponding to the listing of bicycle lane information.\n",
    "\n",
    "    Returns:\n",
    "        bicycle_lanes (Pandas DataFrame): list of dictionaries containing extracted lane information.\n",
    "    \"\"\"\n",
    "    bicycle_lanes_page = requests.get(url).text\n",
    "    bicycle_lanes_list = get_bicycle_lanes_from_page(bicycle_lanes_page)\n",
    "            \n",
    "    bicycle_lanes = pd.DataFrame(bicycle_lanes_list)\n",
    "    bicycle_lanes.loc[bicycle_lanes.borough == 'THE BRONX', 'borough'] = 'BRONX'\n",
    "    bicycle_lanes = bicycle_lanes.rename(columns={'borough': 'BOROUGH', 'begins': 'BEGINS', 'ends': 'ENDS', 'street': 'ON STREET NAME'})\n",
    "    \n",
    "    return bicycle_lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bicycle_lanes = extract_bicycle_lanes(bicycle_lanes_page_url)\n",
    "\n",
    "print bicycle_lanes.head()\n",
    "print len(bicycle_lanes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bicycle lanes are defined by 4 features:\n",
    "\n",
    " 1. Which street they're on (`ON STREET NAME`)\n",
    " 2. Which cross street they begin on (`BEGINS`)\n",
    " 3. Which cross street they end on (`ENDS`)\n",
    " 4. The borough they're on (`BOROUGH`)\n",
    " \n",
    "By using these 4 values, we'll match them to the location of the collisions and add a 'HAS_BIKE_LANE' column to the collisions dataframe, indicating whether the crash occurred on a street that has a bike lane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def merge_bicycle_lanes(df1, df2, columns):\n",
    "    # Populate collisions with bicycle lane data by matching on 'columns'\n",
    "    result = pd.merge(df1, df2, how='left', on=columns)\n",
    "    \n",
    "    # Create a 'HAS_BIKE_LANE' column to indicate the presenc of a bike lane\n",
    "    result = result.assign(HAS_BIKE_LANE = [0 if x is np.nan else 1 for x in result.loc[:, 'BEGINS']])\n",
    "    \n",
    "    # Drop unneeded columns\n",
    "    result = result.drop('BEGINS', axis=1)\n",
    "    result = result.drop('ENDS', axis=1)\n",
    "    \n",
    "    # Multiple bike lanes can be on the same street, disregard that fact\n",
    "    result = result.drop_duplicates()\n",
    "    \n",
    "    # Reset index to account for duplicates\n",
    "    result = result.reset_index(drop=True)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collision = merge_bicycle_lanes(collision, bicycle_lanes, ['ON STREET NAME', 'BOROUGH'])\n",
    "\n",
    "print collision.head()\n",
    "print collision.dtypes\n",
    "print len(collision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to find areas in which collisions occur frequently. This will let us identify collision-prone areas in NYC. All the collisions we have right now have their latitude & longitude information, which is too granular to identify areas. We also have the borough and Zip codes of the locations, but those are not granular enough. What we need to do is to identify 'clusters' from the locations of crashes, which we can use to identify areas. We can use the k-means algorithm to achieve this.\n",
    "\n",
    "Usually, we try to optimize the number of means to give us a good balance of error & generalizability. While we can do that here as well, we can manually choose the number of means to specify how granular we'd like to be with our zoning: choosing a single mean will, obviously, cover the entire NYC metropolitan area but won't provide us with any additional information. By visually observing the results of choosing from a number of means, we can find a good balance of number of zones that translate logically to the layout of the NYC metropolitan area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**NOTE:**\n",
    "Below is our test code for running the k-means algorithm on our dataset. For our final report, we will be identifying and obtaining the zones from our collision data using k-means, assigning zone IDs (the means) to the collisions and use this final version to perform predictions on future data (for example, given a car's features and the zone, predicting the expected number of injuries from a collision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_means_test(X, K_max):\n",
    "    \"\"\"\n",
    "    Finds the best number k for the k_means algorithm according to some features passed to this\n",
    "    method.\n",
    "    \n",
    "    Some code has been taken from: \n",
    "    https://stackoverflow.com/questions/6645895/calculating-the-percentage-of-variance-measure-for-k-means\n",
    "    especially the plotting parts.\n",
    "\n",
    "    Parameters:\n",
    "        X (DataFrame): pandas dataframe containing the features that will be used to divide the\n",
    "        data into clusters.\n",
    "        K-max (Integer): the maximum number of K clusters to test on.\n",
    "\n",
    "    Returns:\n",
    "        Nothing, it just plots the elbow graph that has a marker on the best number of\n",
    "        clusters to use.\n",
    "    \"\"\"\n",
    "    K = range(1, K_max)\n",
    "    KM = [kmeans(X,k) for k in K]\n",
    "    centroids = [cent for (cent,var) in KM]\n",
    "\n",
    "    D_k = [cdist(X, cent, 'euclidean') for cent in centroids]\n",
    "    cIdx = [np.argmin(D,axis=1) for D in D_k]\n",
    "    dist = [np.min(D,axis=1) for D in D_k]\n",
    "    avgWithinSS = [sum(d)/X.shape[0] for d in dist]\n",
    "    \n",
    "    # Choose k\n",
    "    kIdx = min([(v,i) for i,v in enumerate([((x*100) + y)/2.0 for x,y in zip(avgWithinSS, K)])])[1]\n",
    "\n",
    "    # Plot elbow curve\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(K, avgWithinSS, 'b*-')\n",
    "    ax.plot(K[kIdx], avgWithinSS[kIdx], marker='o', markersize=12, \n",
    "            markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Average within-cluster sum of squares')\n",
    "    plt.title('Elbow for K-Means clustering')\n",
    "\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collision1 = collision.drop(['BOROUGH', 'ON STREET NAME', 'CROSS STREET NAME', 'UNIQUE KEY'], axis=1)\n",
    "\n",
    "# Calculate different centroids according to different number of k's.\n",
    "k_means_test(collision1.loc[:,['LATITUDE', 'LONGITUDE']], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
