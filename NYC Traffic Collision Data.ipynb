{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Analyzing NYC Traffic Collision Data</center>\n",
    "## <center>A 15-688 Project by:</center><center><br/> Ahmet Emre Unal (ahmetemu)<br/><br/>Marco Peyrot (mpeyrotc)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New York City is a beautiful city with terrible traffic. The drivers are impatient and aggressive, which results in many traffic collisions every day, some of which, quite unfortunately, lead to injuries and death.\n",
    "\n",
    "For our project, we wanted to understand NYC's most collision-prone areas and try to predict collisions based on many factors, such as location, vehicle type, whether it's a weekday, etc.\n",
    "\n",
    "The [NYPD Motor Vehicle Collisions](https://data.cityofnewyork.us/Public-Safety/NYPD-Motor-Vehicle-Collisions/h9gi-nx95/data) dataset we found was surprisingly feature-rich and populous. It also meant that there were lots of collision instances with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests \n",
    "import scipy\n",
    "from scipy.cluster.vq import kmeans, kmeans2, vq\n",
    "from scipy.spatial.distance import cdist\n",
    "from IPython.core.display import display, HTML\n",
    "from colour import Color\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "matplotlib.use('svg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "raw_data_file_name = 'NYPD_Motor_Vehicle_Collisions.csv'\n",
    "bicycle_lanes_page_url = 'http://www.nyc.gov/html/dot/html/bicyclists/lane-list.shtml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started by cleaning up rows with empty and '`UNKNOWN`' values, throwing away about a third of the original dataset. We then dropped columns that either were unnecessary (like the reason for the crash, since this sort of information is not possible to infer before the collision, like the driver being distracted) or were too detailed (like the vehicle subtypes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    collision = pd.read_csv(file_name, \n",
    "                            na_filter=False, \n",
    "                            parse_dates={'DATE_COMBINED' : ['DATE', 'TIME']}, \n",
    "                            infer_datetime_format=True)\n",
    "    \n",
    "    # Remove rows that don't have the necessary data\n",
    "    columns_to_check_for_empty = ['LOCATION', 'LATITUDE', 'LONGITUDE', 'BOROUGH',\n",
    "                                  'ZIP CODE', 'ON STREET NAME', 'CROSS STREET NAME', \n",
    "                                  'VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 1']\n",
    "    for column in columns_to_check_for_empty:\n",
    "        collision = collision[collision[column] != '']\n",
    "        collision = collision[collision[column] != 'UNKNOWN']\n",
    "\n",
    "    # Drop unneeded columns\n",
    "    columns_to_drop = ['CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2', \n",
    "                       'CONTRIBUTING FACTOR VEHICLE 3', 'CONTRIBUTING FACTOR VEHICLE 4', \n",
    "                       'CONTRIBUTING FACTOR VEHICLE 5', 'LOCATION', 'OFF STREET NAME', \n",
    "                       'VEHICLE TYPE CODE 2', 'VEHICLE TYPE CODE 3', 'VEHICLE TYPE CODE 4', \n",
    "                       'VEHICLE TYPE CODE 5']\n",
    "    for column in columns_to_drop:\n",
    "        collision = collision.drop(column, axis=1)\n",
    "        \n",
    "    # Set column types\n",
    "    collision['ZIP CODE'] = collision['ZIP CODE'].astype(int)\n",
    "    collision['LATITUDE'] = collision['LATITUDE'].astype(float)\n",
    "    collision['LONGITUDE'] = collision['LONGITUDE'].astype(float)\n",
    "    \n",
    "    # Rename date column to just 'DATE'\n",
    "    collision = collision.rename(columns={'DATE_COMBINED':'DATE'})\n",
    "    \n",
    "    # Eliminate duplicates\n",
    "    collision = collision.drop_duplicates()\n",
    "    \n",
    "    # Reset index\n",
    "    collision = collision.reset_index(drop=True)\n",
    "    \n",
    "    return collision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceeded to create two temporal features:\n",
    "\n",
    " 1. Whether it occurred on a weekday or weekend\n",
    " 2. In what 'time period' in the day it occurred\n",
    "\n",
    "The 'time period' requires some explanation: We all know that morning and evening rush hours can be especially more collision-prone than daytime (around noon) and night time (after the evening rush and before the morning rush). We decided to create these four bins with the following time ranges:\n",
    "\n",
    " 1. Night Time: 00:00-06:59 & 20:00-00:00\n",
    " 2. Morning Rush: 07:00-10:59\n",
    " 3. Night Time: 00:00-06:59 & 20:00-00:00\n",
    " 4. Evening Rush: 16:00-19:59\n",
    "\n",
    "Together with the weekday/weekend feature, this allowed us to represent the date in a way that is meaningful to a machine learning algorithm. We also assumed that, as long as it's a weekday, the exact day (whether it's a Monday or a Thursday) is not significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_temporal_features(collision):\n",
    "    collision = _create_time_features(collision)\n",
    "    collision = _create_date_features(collision)\n",
    "    \n",
    "    # We're done with date, we can drop it\n",
    "    collision = collision.drop('DATE', axis=1)\n",
    "    \n",
    "    return collision\n",
    "    \n",
    "\n",
    "def _create_time_features(collision):\n",
    "    # Create one-hot time of day representation\n",
    "    ## Date part is unimportant\n",
    "    morning_rush_begin = pd.datetime(2000, 01, 01, 07, 00, 00).time()\n",
    "    morning_rush_end = pd.datetime(2000, 01, 01, 11, 00, 00).time()\n",
    "    evening_rush_begin = pd.datetime(2000, 01, 01, 16, 00, 00).time()\n",
    "    evening_rush_end = pd.datetime(2000, 01, 01, 20, 00, 00).time()\n",
    "    collision_time = collision['DATE'].dt.time\n",
    "    \n",
    "    ## Night Time 00:00-06:59 & 20:00-00:00\n",
    "    night_time = (collision_time >= evening_rush_end) | (collision_time < morning_rush_begin)\n",
    "    night_time_onehot = pd.get_dummies(night_time).loc[:, True].astype(int)\n",
    "    collision = collision.assign(NIGHT_TIME = night_time_onehot.values)\n",
    "    \n",
    "    ## Morning Rush 07:00-10:59\n",
    "    morning_rush = (collision_time >= morning_rush_begin) & (collision_time < morning_rush_end)\n",
    "    morning_rush_onehot = pd.get_dummies(morning_rush).loc[:, True].astype(int)\n",
    "    collision = collision.assign(MORNING_RUSH = morning_rush_onehot.values)\n",
    "    \n",
    "    ## Night time 00:00-06:59 & 20:00-00:00\n",
    "    day_time = (collision_time >= morning_rush_end) & (collision_time < evening_rush_begin)\n",
    "    day_time_onehot = pd.get_dummies(day_time).loc[:, True].astype(int)\n",
    "    collision = collision.assign(DAY_TIME = day_time_onehot.values)\n",
    "    \n",
    "    ## Evening Rush 16:00-19:59\n",
    "    evening_rush = (collision_time >= evening_rush_begin) & (collision_time < evening_rush_end)\n",
    "    evening_rush_onehot = pd.get_dummies(evening_rush).loc[:, True].astype(int)\n",
    "    collision = collision.assign(EVENING_RUSH = evening_rush_onehot.values)\n",
    "    \n",
    "    return collision\n",
    "\n",
    "def _create_date_features(collision):\n",
    "    # Create one-hot weekday/weekend representation\n",
    "    collision_day = collision['DATE'].dt.dayofweek\n",
    "    \n",
    "    ## Weekday 0, 1, 2, 3, 4\n",
    "    ## Weekend 5, 6\n",
    "    is_weekday = (collision_day <= 4)\n",
    "    is_weekday_onehot = pd.get_dummies(is_weekday).astype(int)\n",
    "    \n",
    "    ## Weekday\n",
    "    weekday_onehot = is_weekday_onehot.loc[:, True]\n",
    "    collision = collision.assign(WEEKDAY = weekday_onehot.values)\n",
    "\n",
    "    ## Weekend\n",
    "    weekend_onehot = is_weekday_onehot.loc[:, False]\n",
    "    collision = collision.assign(WEEKEND = weekend_onehot.values)\n",
    "    \n",
    "    return collision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further proceeded to create a one-hot encoding of the vehicle types that were available in the dataset. NYPD has put almost half of the vehicles in a general group called '`PASSENGER VEHICLE`', which seems to represent the common sedan type of vehicle. Other types, like SUVs, motorcycles, small and large commercial vehicles, have their own types. \n",
    "\n",
    "We removed some collision types (like the collisions in which the vehicle was an '`AMBULANCE`') with very few collisions. This further cut about 5% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vehicle_features(collision):\n",
    "    # Create one-hot vehicle type representation\n",
    "    vehicle_types_onehot = pd.get_dummies(collision.loc[:, 'VEHICLE TYPE CODE 1']).astype(int)\n",
    "    \n",
    "    # Merge Motorcycle & Scooter columns\n",
    "    motorcycle = vehicle_types_onehot.loc[:, 'MOTORCYCLE'] + vehicle_types_onehot.loc[:, 'SCOOTER']\n",
    "    vehicle_types_onehot = vehicle_types_onehot.drop('MOTORCYCLE', axis=1)\n",
    "    vehicle_types_onehot = vehicle_types_onehot.drop('SCOOTER', axis=1)\n",
    "    vehicle_types_onehot = vehicle_types_onehot.assign(MOTORCYCLE = motorcycle.values)\n",
    "    \n",
    "    # Concatanate one-hot with collisions\n",
    "    collision = pd.concat([collision, vehicle_types_onehot], axis=1)\n",
    "    \n",
    "    # Drop unneeded collisions\n",
    "    vehicles_to_drop = ['OTHER', 'AMBULANCE', 'PEDICAB', 'FIRE TRUCK', 'LIVERY VEHICLE']\n",
    "    collisions_to_drop = vehicle_types_onehot.loc[:, vehicles_to_drop[0]]\n",
    "    for i in xrange(1, len(vehicles_to_drop)):  # Start from 1 since we already have OTHER\n",
    "        collisions_to_drop += vehicle_types_onehot.loc[:, vehicles_to_drop[i]]\n",
    "    collisions_to_keep = (collisions_to_drop == 0)\n",
    "    collision = collision[collisions_to_keep]\n",
    "    collision = collision.reset_index(drop=True)  # Reset index due to dropped rows\n",
    "    \n",
    "    # Drop unneeded vehicle columns\n",
    "    for column in vehicles_to_drop:\n",
    "        collision = collision.drop(column, axis=1)\n",
    "    \n",
    "    # Drop vehicle type column\n",
    "    collision = collision.drop('VEHICLE TYPE CODE 1', axis=1)\n",
    "        \n",
    "    return collision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceeded by assigning a 'severity score' to each collision. Each collision, by default, has a severity score of 1. The score increases by 2 for each injury and by 10 for each death. These are arbitrary values we came up with and have no bearing on the cost of life in these collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Default score values\n",
    "INJURY_SCORE = 2\n",
    "DEATH_SCORE = 10\n",
    "\n",
    "def create_severity_score_feature(collision):\n",
    "    # Give all collisions a score of 1 by default\n",
    "    collision['SEVERITY SCORE'] = 1\n",
    "    \n",
    "    # For each injury, add to the score of the collision\n",
    "    collision['SEVERITY SCORE'] += (collision.loc[:, 'NUMBER OF PERSONS INJURED'] * INJURY_SCORE)\n",
    "    \n",
    "    # For each death, add to the score of the collision\n",
    "    collision['SEVERITY SCORE'] += (collision.loc[:, 'NUMBER OF PERSONS KILLED'] * DEATH_SCORE)\n",
    "    \n",
    "    return collision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the collision data\n",
    "collision = load_data(raw_data_file_name)\n",
    "collision = create_temporal_features(collision)\n",
    "collision = create_vehicle_features(collision)\n",
    "collision = create_severity_score_feature(collision)\n",
    "\n",
    "print collision.head()\n",
    "print collision.dtypes\n",
    "print len(collision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed with parsing NYC's [bike lanes list web page](http://www.nyc.gov/html/dot/html/bicyclists/lane-list.shtml) to add another feature to our collision data: whether the street the collision occurred in had a bicycle lane or not. We do that by manually parsing the bicycle lanes page and matching them to the street names of collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_bicycle_lanes_from_page(page_html):\n",
    "    \"\"\"\n",
    "    Parse the table contained in the bicycle lanes webpage.\n",
    "\n",
    "    Args:\n",
    "        page_html (string): String of HTML corresponding to the data source webpage.\n",
    "\n",
    "    Returns:\n",
    "        a dictionary that contains mappings from a category to the list containing the data.\n",
    "        These categories are: street, begins, ends, and borough.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    bicycle_lanes_list = {\n",
    "        'street': [],\n",
    "        'begins': [],\n",
    "        'ends': [],\n",
    "        'borough': []\n",
    "    }\n",
    "\n",
    "    table = soup.findChildren('tbody')[0]\n",
    "    rows = table.findChildren(['tr'])\n",
    "    \n",
    "    for row in rows:\n",
    "        cells = row.findChildren('td')\n",
    "        content = cells[0].text\n",
    "        m = re.search(r'^([a-zA-Z\\s0-9\\-\\.,\\(\\)]+) (from)*(between)* '\n",
    "                      r'([a-zA-Z\\s0-9\\-\\.,\\(\\)]+) (to)*(and)* '\n",
    "                      r'([a-zA-Z\\s0-9\\-\\.,\\(\\)]+)$', content)\n",
    "        \n",
    "        # Content that does not follow this syntax is discarded because\n",
    "        # it refers to landscapes or parks.\n",
    "        if m is not None:\n",
    "            bicycle_lanes_list['street'].append(m.group(1).upper())\n",
    "            bicycle_lanes_list['begins'].append(m.group(4).upper())\n",
    "            bicycle_lanes_list['ends'].append(m.group(7).upper())\n",
    "            bicycle_lanes_list['borough'].append(cells[2].text.upper())\n",
    "\n",
    "    return bicycle_lanes_list\n",
    "    \n",
    "def extract_bicycle_lanes(url):\n",
    "    \"\"\"\n",
    "    Retrieve all of the bicycle lane information for the city of New York.\n",
    "\n",
    "    Parameters:\n",
    "        url (string): page URL corresponding to the listing of bicycle lane information.\n",
    "\n",
    "    Returns:\n",
    "        bicycle_lanes (Pandas DataFrame): list of dictionaries containing extracted lane information.\n",
    "    \"\"\"\n",
    "    bicycle_lanes_page = requests.get(url).text\n",
    "    bicycle_lanes_list = get_bicycle_lanes_from_page(bicycle_lanes_page)\n",
    "            \n",
    "    bicycle_lanes = pd.DataFrame(bicycle_lanes_list)\n",
    "    bicycle_lanes.loc[bicycle_lanes.borough == 'THE BRONX', 'borough'] = 'BRONX'\n",
    "    columns_to_rename = {'borough': 'BOROUGH', 'begins': 'BEGINS', 'ends': 'ENDS', 'street': 'ON STREET NAME'}\n",
    "    bicycle_lanes = bicycle_lanes.rename(columns=columns_to_rename)\n",
    "    \n",
    "    return bicycle_lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bicycle_lanes = extract_bicycle_lanes(bicycle_lanes_page_url)\n",
    "\n",
    "print bicycle_lanes.head()\n",
    "print len(bicycle_lanes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bicycle lanes are defined by four features:\n",
    "\n",
    " 1. Which street they're on (`ON STREET NAME`)\n",
    " 2. Which cross street they begin on (`BEGINS`)\n",
    " 3. Which cross street they end on (`ENDS`)\n",
    " 4. The borough they're on (`BOROUGH`)\n",
    " \n",
    "By using these four values, we'll match them to the location of the collisions and add a 'HAS_BIKE_LANE' column to the collisions data frame, indicating whether the crash occurred on a street that has a bike lane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def merge_bicycle_lanes(df1, df2, columns):\n",
    "    # Populate collisions with bicycle lane data by matching on 'columns'\n",
    "    result = pd.merge(df1, df2, how='left', on=columns)\n",
    "    \n",
    "    # Create a 'HAS_BIKE_LANE' column to indicate the presenc of a bike lane\n",
    "    result = result.assign(HAS_BIKE_LANE = [0 if x is np.nan else 1 for x in result.loc[:, 'BEGINS']])\n",
    "    \n",
    "    # Drop unneeded columns\n",
    "    result = result.drop('BEGINS', axis=1)\n",
    "    result = result.drop('ENDS', axis=1)\n",
    "    \n",
    "    # Multiple bike lanes can be on the same street, disregard that fact\n",
    "    result = result.drop_duplicates()\n",
    "    \n",
    "    # Reset index to account for duplicates\n",
    "    result = result.reset_index(drop=True)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collision = merge_bicycle_lanes(collision, bicycle_lanes, ['ON STREET NAME', 'BOROUGH'])\n",
    "\n",
    "print collision.head()\n",
    "print collision.dtypes\n",
    "print len(collision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to find areas in which collisions occur frequently. This will let us identify collision-prone areas in NYC. All the collisions we have right now have their latitude & longitude information, which is too granular to identify areas. We also have the borough and Zip codes of the locations, but those are not granular enough. What we need to do is to define 'clusters' from the locations of crashes, which we can use to determine areas. We can use the k-means algorithm to achieve this.\n",
    "\n",
    "Usually, we try to optimize the number of means to give us a good balance of error & generalizability. While we can do that here as well, we can manually choose the number of means to specify how granular we'd like to be with our zoning: selecting a single mean will, naturally, cover the entire NYC metropolitan area but won't provide us with any additional information. By visually observing the results of choosing from a number of means, we can find a good balance of the number of zones that translate logically to the layout of the NYC metropolitan area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**NOTE:**\n",
    "Below is our test code for running the k-means algorithm on our dataset. For our final report, we will be identifying and obtaining the zones from our collision data using k-means, assigning zone IDs (the means) to the collisions and use this final version to perform predictions on future data (for example, given a car's features and the zone, predicting the expected number of injuries from a collision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_means_test(X, K_max):\n",
    "    \"\"\"\n",
    "    Finds the best number k for the k_means algorithm according to some features passed to this\n",
    "    method.\n",
    "    \n",
    "    Some code has been taken from: \n",
    "    https://stackoverflow.com/questions/6645895/calculating-the-percentage-of-variance-measure-for-k-means\n",
    "    especially the plotting parts.\n",
    "\n",
    "    Parameters:\n",
    "        X (DataFrame): pandas dataframe containing the features that will be used to divide the\n",
    "        data into clusters.\n",
    "        K-max (Integer): the maximum number of K clusters to test on.\n",
    "\n",
    "    Returns:\n",
    "        Nothing, it just plots the elbow graph that has a marker on the best number of\n",
    "        clusters to use.\n",
    "    \"\"\"\n",
    "    K = range(1, K_max)\n",
    "    KM = [kmeans(X,k) for k in K]\n",
    "    centroids = [cent for (cent,var) in KM]\n",
    "\n",
    "    D_k = [cdist(X, cent, 'euclidean') for cent in centroids]\n",
    "    cIdx = [np.argmin(D,axis=1) for D in D_k]\n",
    "    dist = [np.min(D,axis=1) for D in D_k]\n",
    "    avgWithinSS = [sum(d)/X.shape[0] for d in dist]\n",
    "    \n",
    "    # Choose k\n",
    "    kIdx = min([(v,i) for i,v in enumerate([((x*100) + y)/2.0 for x,y in zip(avgWithinSS, K)])])[1]\n",
    "\n",
    "    # Plot elbow curve\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(K, avgWithinSS, 'b*-')\n",
    "    ax.plot(K[kIdx], avgWithinSS[kIdx], marker='o', markersize=12, \n",
    "            markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Average within-cluster sum of squares')\n",
    "    plt.title('Elbow for K-Means clustering')\n",
    "\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collision1 = collision.drop(['BOROUGH', 'ON STREET NAME', 'CROSS STREET NAME', 'UNIQUE KEY'], axis=1)\n",
    "\n",
    "# Calculate different centroids according to different number of k's.\n",
    "k_means_test(collision1.loc[:,['LATITUDE', 'LONGITUDE']], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = collision1.loc[:,['LATITUDE', 'LONGITUDE']]\n",
    "centroids, labels = kmeans2(X,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createPage(title, script):\n",
    "    \"\"\"\n",
    "    Creates a webpage that will hold the map where the points of each cluster are plotted. This page\n",
    "    shows an instance of Leaflet (http://leafletjs.com/).\n",
    "\n",
    "    Parameters:\n",
    "        title (string): the name of the webpage that will appear on the browser\n",
    "        script (string): the location of the file that contains the plotting information (must be a\n",
    "                        javascript file)\n",
    "\n",
    "    Returns:\n",
    "        The HTML code that forms the webpage.\n",
    "    \"\"\"\n",
    "    \n",
    "    page = \"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "            <head>\n",
    "                <title>TITLE</title>\n",
    "                <meta charset=\"utf-8\" />\n",
    "                <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "\n",
    "                <link rel=\"stylesheet\" href=\"https://unpkg.com/leaflet@1.0.2/dist/leaflet.css\" />\n",
    "                <script src=\"https://unpkg.com/leaflet@1.0.2/dist/leaflet.js\"></script>\n",
    "            </head>\n",
    "            <body>\n",
    "                <div id=\"mapid\" style=\"width: 800px; height: 600px;\"></div>\n",
    "                <script src=\"SCRIPT\"></script>\n",
    "            </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "    \n",
    "    page = page.replace(\"TITLE\", title)\n",
    "    page = page.replace(\"SCRIPT\", script)\n",
    "    \n",
    "    return page\n",
    "\n",
    "def createScript(name, lat, lon, zoom):\n",
    "    \"\"\"\n",
    "    Creates a javascript file with the basic content to display a map in a webpage.\n",
    "\n",
    "    Parameters:\n",
    "        name (string): the name (or directory) of the javascript file to be created\n",
    "        lat (float): the latitude where the map will be centered\n",
    "        lon (float): the longitude where the map will be centered\n",
    "        zoom (int): the zoom level in which the map starts (can be changed in browser)\n",
    "\n",
    "    Returns:\n",
    "        Nothing, the file is written onto disk\n",
    "    \"\"\"\n",
    "    \n",
    "    script = \"\"\"\n",
    "    var mymap = L.map('mapid').setView([LAT, LON], ZOOM);\n",
    "    \n",
    "    L.tileLayer('https://api.tiles.mapbox.com/v4/{id}/{z}/{x}/{y}.png?access_token=pk.eyJ1IjoibWFwYm94IiwiYSI6ImNpandmbXliNDBjZWd2M2x6bDk3c2ZtOTkifQ._QA7i5Mpkd_m30IGElHziw', {\n",
    "        maxZoom: 18,\n",
    "        attribution: 'Map data &copy; <a href=\"http://openstreetmap.org\">OpenStreetMap</a> contributors, ' +\n",
    "            '<a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">CC-BY-SA</a>, ' +\n",
    "            'Imagery © <a href=\"http://mapbox.com\">Mapbox</a>',\n",
    "        id: 'mapbox.streets'\n",
    "    }).addTo(mymap);\n",
    "    \n",
    "    \"\"\"\n",
    "    script = script.replace(\"LAT\", str(lat))\n",
    "    script = script.replace(\"LON\", str(lon))\n",
    "    script = script.replace(\"ZOOM\", str(zoom))\n",
    "    \n",
    "    with open(name + '.js', \"w\") as script_file:\n",
    "        script_file.write(script)\n",
    "        \n",
    "def appendScript(name, lats, lngs, color, size):\n",
    "    \"\"\"\n",
    "    Appends to a javascript file all the points to be plotted with a given color and size\n",
    "\n",
    "    Parameters:\n",
    "        name (string): the name (or directory) of the javascript file to be used\n",
    "        lats (list[float]): the list of latitudes to be plotted\n",
    "        lngs (list[float]): the list of longitudes to be plotted\n",
    "        color (Color): the color in which the points will be plotted\n",
    "        size (int): the size of each point\n",
    "\n",
    "    Returns:\n",
    "        Nothing, the file is written onto disk\n",
    "    \"\"\"\n",
    "    \n",
    "    point = \"\"\"\n",
    "    L.circle([LAT, LON], SIZE, {\n",
    "        color: 'COLOR',\n",
    "        fillColor: 'COLOR',\n",
    "        fillOpacity: 0.5\n",
    "    }).addTo(mymap);\n",
    "    \"\"\"\n",
    "    \n",
    "    point = point.replace('SIZE', str(size))\n",
    "    point = point.replace('COLOR', str(color.hex))\n",
    "        \n",
    "    with open(name + '.js', \"a\") as script_file:\n",
    "        for lat, lon in zip(lats, lngs):\n",
    "            new_point = point.replace('LAT', str(lat))\n",
    "            new_point = new_point.replace('LON', str(lon)) \n",
    "            script_file.write(new_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotPoints(X, labels, K, script, html_file, num_points, lat=40.723671, lon=-74.009725, zoom=11):\n",
    "    \"\"\"\n",
    "    Creates a html file and a javascript file that will hold a map of a region centered at the specified latitude\n",
    "    and logitude coordinates. It will plot several points based on the labels passed on to it and the number\n",
    "    of clusters they map to.\n",
    "    \n",
    "    Parameters:\n",
    "        X (pandas df): a dataframe that contains latitude and longitude values\n",
    "        labels (list): holds the label for which each row in X is assigned to a cluster\n",
    "        script (string): the name (or directory) of the javascript file to be used\n",
    "        html_file (string): the name (or directory) of the html file to be used\n",
    "        num_points (int): the maximum number of points to be plotted per cluster\n",
    "        lat (float): the latitude where the map will be centered\n",
    "        lon (float): the longitude where the map will be centered\n",
    "        zoom (int): the zoom level in which the map starts (can be changed in browser)\n",
    "        \n",
    "    Returns:\n",
    "        Nothing, the files are written onto disk\n",
    "    \"\"\"\n",
    "    purple = Color(\"purple\")\n",
    "    colors = list(purple.range_to(Color(\"green\"),K))\n",
    "    \n",
    "    # Append labels to coordinates\n",
    "    X['label'] = labels\n",
    "    \n",
    "    with open(html_file + '.html', \"w\") as map_file:\n",
    "        map_file.write(createPage(html_file, script + '.js'))\n",
    "        \n",
    "    createScript(script, lat, lon, zoom)\n",
    "    \n",
    "    for label in set(labels):\n",
    "        assignments = X.loc[X.label == label, :]\n",
    "        assignments = assignments.iloc[np.random.permutation(range(0,assignments.shape[0])), :].iloc[range(0,num_points), :]\n",
    "        appendScript(script, assignments.loc[:,'LATITUDE'], assignments.loc[:,'LONGITUDE'], colors[label], 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotPoints(X, labels, 4, 'file', 'page', 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_training_sets(subset):\n",
    "    X = subset\n",
    "    Y = X['SEVERITY SCORE']\n",
    "\n",
    "    borough_onehot = pd.get_dummies(X['BOROUGH']).astype(int)\n",
    "    X = pd.concat([X, borough_onehot], axis=1)\n",
    "    X = X.drop(['BOROUGH', 'ZIP CODE', 'LATITUDE', 'LONGITUDE', 'ON STREET NAME', 'CROSS STREET NAME', 'SEVERITY SCORE', 'UNIQUE KEY'], axis=1)\n",
    "    return X, Y\n",
    "    \n",
    "num_collisions_total = collision.shape[0]\n",
    "num_collisions_test = 1000\n",
    "num_collisions_training = 10000 # num_collisions_total - num_collisions_test\n",
    "    \n",
    "collision_random_order = collision.loc[np.random.choice(num_collisions_total, (num_collisions_training + num_collisions_test), replace=False), :]\n",
    "X_tr, Y_tr = get_training_sets(collision_random_order[:num_collisions_training])\n",
    "X_te, Y_te = get_training_sets(collision_random_order[num_collisions_training:])\n",
    "    \n",
    "from sklearn import linear_model\n",
    "# https://stackoverflow.com/questions/25260000/scikit-learns-gridsearchcv-stops-working-when-n-jobs1\n",
    "# import multiprocessing as mp; mp.set_start_method('forkserver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clsf = linear_model.LogisticRegressionCV(verbose=0)\n",
    "clsf.fit_transform(X_tr, Y_tr)\n",
    "print clsf.score(X_te, Y_te)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
